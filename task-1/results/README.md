# Task1

## Нефункциональные требования

- Разрабатываемое решение должно поддерживать различные источники данных: SQL БД, CSV файлы, системы очередей и т.п.
- Требуется интеграция с внешними API, а также BigQuery, Redshift, Kafka и Spark
- Ожидаемых объём данных около 1 млн записей за один запуск пайплайна
- Требуется поддержка ветвления в рамках пайплана, а также возможность перезапускать задачи в случае ошибок


## Решение

- В качестве наиболее походящего решения выбран Apache Airflow.
- Airflow удовлетворяет всем заявленным требованиям, позволяет создавать ветвления, имеет готовую интеграцию с разными
  внешними системами, позволяет организовывать робастные задачи, а также обладает большими возможностями для мониторинга.
- В случае необходимости выполнить интеграцию с какой-либо нестандартной внешней системой Airflow поддерживает создание
  кастомных интеграций.
- Для возможности гибко масштабировать решение следует развернуть Airflow в рамках кластера k8s и использовать 
  KubernetesExecutor для выполняемых задач.

## Альтернативы

- Использование cron jobs видится не оправданным, так как не позволит создавать гибкие пайплайны с ветвлением, которые
  при этом будет удобно поддерживать и мониторить.
- Использовать Event-driven подхода для запуска задач обработки также выглядит неподходящим. Использование такого
  подхода позволяет хорошо выполнять масштабирование, однако при этом общая наблюдаемость выполнения пайплайно будет
  существенно ниже чем в ситуации с Airflow, а также потребует реализации дополнительных систем (например, поддержки
  ретраев), которые доступны в Airflow из коробки.
